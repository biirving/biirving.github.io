<!DOCTYPE html>
<html>
  <head>
    <title>Ben Irving</title>
    <style>
      title {
        font-family: Arial, sans-serif;
      }
      p {
        font-family: Arial, sans-serif;
      }
      h1, h2 {
        font-family: Arial, Helvetica, sans-serif;
      }
      ul {
        font-family: Arial, Helvetica, sans-serif;
      }
      .large-bold-h2 {
        font-size: 24px;
        font-weight: bold;
      }
      .citation {
        font-style: italic;
        margin-top: 10px;
      }
      .citation-link {
        display: block;
        margin-top: 5px;
        text-decoration: none;
        color: blue;
      }
    </style>
  </head>
  <body>
    
    <h1>Ben Irving</h1>
    <p>My name is Ben, and I am currently a computer science student at Northeastern University interested in generative models.</p>
    <h2>Projects</h2>
    <p>Here are some projects I have done, and some I am still working on:</p>
    <ul>
      <h2 class="large-bold-h2">Equivariant Proximal Policy Optimization With Behavioral Cloning</h2>
      <p>
        Equivariance has been shown to increase sample efficiency in many different reinforcement learning algorithms. 
        These models are particularly relevant for classic control and robotic manipulation learning problems,
        where state spaces can be thought of as symmetric under rotation. This paper builds on previous work,
        examining the model architecture for equivariant actor-critic methods, and how symmetry can burnish the 
        Proximal Policy Optimization (PPO) algorithm. The code is available at <a href="https://github.com/biirving/aur_ppo" target="_blank">AUR_PPO GitHub Repository</a>
      </p>
      <img src="robot_arm" alt="A snippet of the environment">
    </ul>
    <ul>
      <h2 class="large-bold-h2">"Better Together", Large Graph Embeddings with Scalable representation Learning</h2>
      <p>
        Last summer, I worked on Ken Church's team at the JSALT Speech and NLP workshop hosted by Johns Hopkins. Our overall aim was to build
        an academic search engine for papers and authors in Semantic Scholar (S2).
        My focus was on large-scale graph embeddings, implementing and refining traditional linear algebra methods
        and graph neural networks to produce embedding files on CPU and GPU. I focused on the PRONe algorithm (Zhang et al., 2019), which utilizes spectral clustering, Chebyshev iterations and fourier transforms to produce embeddings.
        I worked in python and C, toying with the low-level linear algebra libraries to optimize compute efficiency on our limited hardware. 
        The demo is available at <a href="http://34.204.188.58//similar.html" target="_blank">Better Together Demo</a>, and the code is here 
        <a href="https://github.com/kwchurch/JSALT_Better_Together" target="_blank">Better Together Repository</a>.
        <img src="better_together" alt="Here is our search engine computing related works for Attention is All You Need (Vaswani et al., 2017)">

      </p>
    </ul>
  </body>
</html>
      <li>WIP: Multimodal encoding antecedent information.</li>
    </ul>
    <!-- Add this link to your "Essays" page -->
    <a href="essays.html">Essays and Stories</a>
  </body>
</html>

